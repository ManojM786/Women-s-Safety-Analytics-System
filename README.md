ğŸ“Œ **Pinned File:** [Women.ipynb](Women.ipynb)

# Women Safety Analytics System ğŸš€

An AI-powered **video surveillance system** that processes real-time video to detect persons and vehicles, classify gender, recognize emotions, and detect potential **SOS situations** using advanced AI models.

ğŸ”— **Jupyter Notebook**: [Women.ipynb](https://github.com/ManojM786/Women-Safety-Analytics-System/blob/main/Women.ipynb) (Click to view the full implementation)

---



## ğŸ“Œ Features
âœ… **Person & Vehicle Detection** using **YOLOv8**  
âœ… **Gender Classification** using **EfficientNet** (trained on PETA dataset)  
âœ… **Facial Emotion Recognition** using **FER Model**  
âœ… **Automatic Number Plate Recognition (ANPR)** using **EasyOCR**  
âœ… **SOS Gesture Recognition** using **MediaPipe Pose & Hands**  
âœ… **Lone Woman Tracking** for enhanced safety analytics  
âœ… **Processed Output Video & Reports**  

---

## ğŸš€ Algorithm

### ğŸ›  Step 1: Initialization  
- Load **YOLOv8** for detecting persons and cars.  
- Load **EfficientNet Gender Classifier** (trained on PETA dataset).  
- Load **YOLO Face Detector** (for emotion recognition).  
- Load **DeepSORT** for object tracking.  
- Load **Facial Emotion Recognition (FER) Model**.  
- Initialize **EasyOCR** for **Automatic Number Plate Recognition (ANPR)**.  
- Set up **MediaPipe Pose & Hands** for **SOS gesture recognition**.  

### ğŸ¥ Step 2: Process Video Frame-by-Frame  
- Read each frame from the input video.  
- Run **YOLOv8** to detect objects (**persons & cars**).  
- Pass detected objects to **DeepSORT** to track them over time.  

### ğŸ§‘â€ğŸ¤– Step 3: Process Each Tracked Object  
#### ğŸ‘¥ If the object is a person:  
âœ”ï¸ Classify **Gender** (Man/Woman) using **EfficientNet**.  
âœ”ï¸ Store **track ID & gender** to ensure consistency.  
âœ”ï¸ Draw Bounding Boxes:  
  - ğŸŸ¦ **Blue** for **Men**  
  - ğŸŸ© **Green** for **Women**  

âœ”ï¸ If the person is a **woman**:  
  - ğŸ­ Detect **face** using YOLO (if visible).  
  - ğŸ­ Classify **facial emotion** using FER.  
  - ğŸš¨ If emotion is **Sad, Fear, Disgust, or Angry** â†’ **Perform SOS detection**.  

#### ğŸš— If the object is a car:  
âœ”ï¸ Extract the bounding box.  
âœ”ï¸ Run **ANPR** (Automatic Number Plate Recognition).  
âœ”ï¸ Store detected **license plate numbers**.  

### ğŸš¨ Step 4: SOS Gesture Recognition for Women  
ğŸ“Œ **Performed only if emotion is negative.**  
- Extract **body pose** using **MediaPipe Pose Estimation**.  
- Detect **hand gestures** using **MediaPipe Hands** (**Raised Hand, Waving, etc.**).  
- Detect **running or sudden movements**.  
âœ”ï¸ **If at least 2 out of 3 conditions are met â†’ Mark as "SOS Detected".**  
âœ”ï¸ **Draw ğŸŸ¥ Red bounding box** & log the SOS event.  

### ğŸ‘¤ Step 5: Lone Woman Tracking  
- Track **women who are alone** for **10+ seconds**.  
- If a woman remains alone beyond the threshold, **flag her as Lone Woman**.  

### ğŸ“„ Step 6: Generate Outputs  
ğŸ“Œ **Processed Output Video** with bounding boxes.  
ğŸ“Œ **Report (`report.txt`)** including:  
  - Total **Men & Women detected**  
  - Total **SOS cases**  
  - Total **License plates identified**  
  - **Lone women tracking results**  
ğŸ“Œ Car **license plate data** saved in (`car_results.csv`).  

### ğŸ Step 7: Final Processing & Cleanup  
- Release video resources.  
- Print **"Detection Completed"** message.  

---

## ğŸ’¡ Key Features & Optimizations  
âœ… **Gender classification happens immediately** after person detection (**EfficientNet**).  
âœ… **Face detection is used ONLY for emotion recognition** (**not gender classification**).  
âœ… **DeepSORT ID consistency is improved** (`max_age=90`) to prevent misclassification.  

---

## ğŸ“¥ Installation  

**Clone the Repository**
```bash
git clone https://github.com/ManojM786/Women-Safety-Analytics-System.git
cd Women-Safety-Analytics-System
```

# Create and Activate a Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```
# Install Dependencies

```bash
pip install -r requirements.txt
```
# ğŸ”§ Usage
## Run the Main Notebook

```bash

jupyter notebook Women.ipynb
```
Follow the notebook cells to execute:

âœ… Object detection & tracking
âœ… Gender classification
âœ… Facial emotion recognition
âœ… SOS detection & lone woman tracking

# ğŸ“Š Results

This system processes real-time video and generates reports detailing:

**âœ… Number of detected men & women
âœ… Identified SOS cases
âœ… Detected vehicle license plates
âœ… Tracking lone women in public spaces**


All results are stored in:

**âœ…Processed output video
âœ…Report file (report.txt)
âœ…License plate data (car_results.csv)** 


# ğŸ¤ Contributing

Contributions are welcome! If you want to enhance the system, follow these steps:

**1. Fork the Repository
2. Create a New Branch**
```bash
git checkout -b feature-branch
```
**3. Commit Your Changes**
```bash
git commit -m "Added new feature"
```
**4. Push the Branch**
```bash
git push origin feature-branch
```
**5. Open a Pull Request describing your changes.**


# ğŸ“œ License

This project is licensed under the MIT License. See the LICENSE file for details.

# ğŸ“© Contact
For any questions or feedback, feel free to reach out:
**ğŸ“§ Email: manojdatascientist7@gmail.com
ğŸ”— GitHub: ManojM786**
